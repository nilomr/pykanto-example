{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "%matplotlib inline\n",
    "import git\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from argparse import ArgumentParser\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter\n",
    "from copy import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import neptune.new as neptune\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "# torch and lightning imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from neptune.new.types import File\n",
    "from PIL import Image, ImageEnhance\n",
    "from pytorch_lightning.callbacks.finetuning import BaseFinetuning\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "from pytorch_lightning.utilities.rank_zero import rank_zero_info\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchmetrics import Accuracy, ConfusionMatrix\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import _log_api_usage_once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure that all operations are deterministic for reproducibility\n",
    "seed = 42\n",
    "pl.seed_everything(seed)\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class and method definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General-purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(spectrogram: np.ndarray, pad_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Centre pads an RGB spectrogram to a given length.\n",
    "\n",
    "    Args:\n",
    "        spectrogram (np.ndarray): Spectrogram to pad.\n",
    "        pad_length (int): Full length of padded spectrogram\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Padded spectrogram\n",
    "    \"\"\"\n",
    "    spec_shape = np.shape(spectrogram)\n",
    "    excess_needed = pad_length - spec_shape[1]\n",
    "    pad_left = int(np.floor(float(excess_needed) / 2))\n",
    "    pad_right = int(np.ceil(float(excess_needed) / 2))\n",
    "    padded_spec = np.full((spec_shape[0], pad_length, 3), np.min(spectrogram))\n",
    "    padded_spec[:, pad_left : pad_length - pad_right, :] = spectrogram\n",
    "    return padded_spec\n",
    "\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data augmentation classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeCrop(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Crops the given image at a random point in the time domain\n",
    "    greater than its height and smaller than its maximum\n",
    "    length minus its height.\n",
    "\n",
    "    Note:\n",
    "        Does not work with tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be cropped.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image: Cropped image.\n",
    "        \"\"\"\n",
    "        img = np.asarray(img)\n",
    "        H, W = img.shape[:2]\n",
    "        if W < H:\n",
    "            # Pads adding some extra width\n",
    "            # so that the img is not always\n",
    "            # in the same position\n",
    "            img = pad(img, int(H + H * 0.25))\n",
    "\n",
    "        H, W = img.shape[:2]\n",
    "        r_idx = random.randint(0, W - H)\n",
    "        img = img[:, r_idx : r_idx + H]\n",
    "        return Image.fromarray(img)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\"\n",
    "\n",
    "\n",
    "class ChangeBrightness(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Changes the brightness by a factor drawn from a uniform\n",
    "    distribution between provided numbers.\n",
    "\n",
    "    Args:\n",
    "        factor (tuple): A tuple containing a range of\n",
    "            brightness (e.g. 0.5 means 50% brightness).\n",
    "        p (float): Probability with which the\n",
    "            transformation will be applied.\n",
    "\n",
    "    Warning:\n",
    "        Does not work with tensors.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, factor: tuple = (0.9, 1.6), p: float = 0.5):\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        self.factor = factor\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be modified.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image: Modified image.\n",
    "        \"\"\"\n",
    "        if self.p < torch.rand(1):\n",
    "            return img\n",
    "        f = random.uniform(self.factor[0], self.factor[1])\n",
    "        enhancer = ImageEnhance.Brightness(img)\n",
    "        img = enhancer.enhance(f)\n",
    "        return img\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(factor={self.factor}, p={self.p})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/val/test transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgTransform:\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225),\n",
    "    ):\n",
    "        self.stage = {\n",
    "            \"train\": T.Compose(\n",
    "                [\n",
    "                    TimeCrop(),\n",
    "                    T.RandomRotation(degrees=(-2, 2)),\n",
    "                    T.RandomAdjustSharpness(sharpness_factor=6, p=0.2),\n",
    "                    T.GaussianBlur(kernel_size=(3, 3), sigma=(0.005, 4)),\n",
    "                    ChangeBrightness(factor=(0.8, 1.6), p=0.5),\n",
    "                    T.ToTensor(),\n",
    "                    # T.Normalize(mean, std),\n",
    "                    T.RandomErasing(\n",
    "                        p=0.2, scale=(0.02, 0.05), ratio=(0.3, 3.3)\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "            \"validate\": T.Compose(\n",
    "                [\n",
    "                    TimeCrop(),\n",
    "                    T.ToTensor(),\n",
    "                    # T.Normalize(mean, std),\n",
    "                ]\n",
    "            ),\n",
    "            \"test\": T.Compose(\n",
    "                [\n",
    "                    TimeCrop(),\n",
    "                    T.ToTensor(),\n",
    "                    # T.Normalize(mean, std),\n",
    "                ]\n",
    "            ),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreatTitDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_path, test_path, batch_size=16, seed=42):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.seed = seed\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # Load and split training set\n",
    "        d = ImageFolder(self.train_path)\n",
    "\n",
    "        # Prepare weighted sampler for training data (oversample)\n",
    "        class_count = Counter(d.targets)\n",
    "        class_weights = torch.Tensor(\n",
    "            [\n",
    "                len(d.targets) / c\n",
    "                for c in pd.Series(class_count).sort_index().values\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        sample_weights = [0] * len(d)\n",
    "        for idx, (image, label) in enumerate(d):\n",
    "            class_weight = class_weights[label]\n",
    "            sample_weights[idx] = class_weight\n",
    "\n",
    "        self.train_sampler = WeightedRandomSampler(\n",
    "            weights=sample_weights, num_samples=len(d), replacement=True\n",
    "        )\n",
    "\n",
    "        # Stratified split for validation\n",
    "        train_idx, valid_idx = train_test_split(\n",
    "            np.arange(len(d.targets)),\n",
    "            test_size=0.2,\n",
    "            shuffle=True,\n",
    "            random_state=self.seed,\n",
    "            stratify=d.targets,\n",
    "        )\n",
    "\n",
    "        # Prepare train/validation/test datasets\n",
    "        self.train, self.validate = copy(d), copy(d)\n",
    "        self.train.imgs = np.array(d.imgs)[train_idx].tolist()\n",
    "        self.train.targets = np.array(d.targets)[train_idx].tolist()\n",
    "        self.validate.imgs = np.array(d.imgs)[valid_idx].tolist()\n",
    "        self.validate.targets = np.array(d.targets)[valid_idx].tolist()\n",
    "        self.test = ImageFolder(self.test_path)\n",
    "\n",
    "        # Transforms\n",
    "        self.train.transform = ImgTransform().stage[\"train\"]\n",
    "        self.validate.transform = ImgTransform().stage[\"validate\"]\n",
    "        self.test.transform = ImgTransform().stage[\"test\"]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "            sampler=self.train_sampler,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.validate,\n",
    "            batch_size=100,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test,\n",
    "            batch_size=100,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See [1] for more details on the training regime:\n",
    "\n",
    "class MilestonesFinetuning(BaseFinetuning):\n",
    "    def __init__(self, milestones: tuple = (5, 10), train_bn: bool = False):\n",
    "        super().__init__()\n",
    "        self.milestones = milestones\n",
    "        self.train_bn = train_bn\n",
    "\n",
    "    def freeze_before_training(self, pl_module: pl.LightningModule):\n",
    "        self.freeze(modules=pl_module.feature_extractor, train_bn=self.train_bn)\n",
    "\n",
    "    def finetune_function(\n",
    "        self,\n",
    "        pl_module: pl.LightningModule,\n",
    "        epoch: int,\n",
    "        optimizer: Optimizer,\n",
    "        opt_idx: int,\n",
    "    ):\n",
    "        if epoch == self.milestones[0]:\n",
    "            # unfreeze 5 last layers\n",
    "            self.unfreeze_and_add_param_group(\n",
    "                modules=pl_module.feature_extractor[-5:],\n",
    "                optimizer=optimizer,\n",
    "                train_bn=self.train_bn,\n",
    "            )\n",
    "\n",
    "        elif epoch == self.milestones[1]:\n",
    "            # unfreeze remaining layers\n",
    "            self.unfreeze_and_add_param_group(\n",
    "                modules=pl_module.feature_extractor[:-5],\n",
    "                optimizer=optimizer,\n",
    "                train_bn=self.train_bn,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main model module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logg_params = {\n",
    "    \"on_step\": True,\n",
    "    \"on_epoch\": True,\n",
    "    \"prog_bar\": True,\n",
    "    \"logger\": True,\n",
    "}\n",
    "\n",
    "class ResNetClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int | None = None,\n",
    "        backbone: str = \"resnet50\",\n",
    "        train_bn: bool = False,\n",
    "        batch_size: int = 16,\n",
    "        transfer=True,\n",
    "        milestones: tuple = (2, 4),\n",
    "        lr: float = 1e-3,\n",
    "        lr_scheduler_gamma: float = 1e-1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.__dict__.update(locals())\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = backbone\n",
    "        self.transfer = transfer = (True,)\n",
    "        self.lr = lr\n",
    "        self.milestones = milestones\n",
    "        self.lr_scheduler_gamma = lr_scheduler_gamma\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.__build_model()\n",
    "\n",
    "        self.train_acc = Accuracy()\n",
    "        self.valid_acc = Accuracy()\n",
    "        self.test_acc = Accuracy()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.val_confusion = ConfusionMatrix(num_classes=self.num_classes)\n",
    "        self.test_confusion = ConfusionMatrix(num_classes=self.num_classes)\n",
    "\n",
    "    def __build_model(self):\n",
    "        \"\"\"Define model layers & loss.\"\"\"\n",
    "\n",
    "        # 1. Load pre-trained network:\n",
    "        model_func = getattr(models, self.backbone)\n",
    "        self.backbone = model_func(pretrained=self.transfer)\n",
    "\n",
    "        _layers = list(self.backbone.children())[:-1]\n",
    "        self.feature_extractor = nn.Sequential(*_layers)\n",
    "        linear_size = list(self.backbone.children())[-1].in_features\n",
    "\n",
    "        # 2. Classifier:\n",
    "        # _fc_layers = [nn.Linear(2048, 1000), nn.ReLU(), nn.Linear(1000, self.num_classes)]\n",
    "        # self.backbone.fc = nn.Sequential(*_fc_layers)\n",
    "        self.backbone.fc = nn.Linear(linear_size, self.num_classes)\n",
    "\n",
    "        # 3. Loss:\n",
    "        self.loss_func = (\n",
    "            nn.BCEWithLogitsLoss()\n",
    "            if self.num_classes == 2\n",
    "            else nn.CrossEntropyLoss()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        parameters = list(self.parameters())\n",
    "        trainab_params = list(filter(lambda p: p.requires_grad, parameters))\n",
    "        rank_zero_info(\n",
    "            f\"The model will start training with only {len(trainab_params)} \"\n",
    "            f\"trainable parameters out of {len(parameters)}.\"\n",
    "        )\n",
    "        optimizer = optim.Adam(trainab_params, lr=self.lr)\n",
    "        scheduler = MultiStepLR(\n",
    "            optimizer, milestones=self.milestones, gamma=self.lr_scheduler_gamma\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "\n",
    "        loss = self.loss_func(preds, y)\n",
    "        self.train_acc(preds, y)\n",
    "        self.log(\"train/loss\", loss, **logg_params)\n",
    "        self.log(\"train/acc\", self.train_acc, **logg_params)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "\n",
    "        loss = self.loss_func(preds, y)\n",
    "        self.train_acc(preds, y)\n",
    "        self.log(\"val/loss\", loss, **logg_params)\n",
    "        self.log(\"val/acc\", self.train_acc, **logg_params)\n",
    "        self.val_confusion.update(preds, batch[1])\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "\n",
    "        loss = self.loss_func(preds, y)\n",
    "        self.test_acc(preds, y)\n",
    "        self.log(\"test/loss\", loss, **logg_params)\n",
    "        self.log(\"test/acc\", self.test_acc, **logg_params)\n",
    "        self.test_confusion.update(preds, batch[1])\n",
    "\n",
    "        # Save wrong prediction images\n",
    "        y_true = y.cpu().detach().numpy()\n",
    "        y_pred = preds.argmax(axis=1).cpu().detach().numpy()\n",
    "\n",
    "        for j in np.where(np.not_equal(y_true, y_pred))[0]:\n",
    "            img = np.squeeze(x[j].cpu().detach().numpy())\n",
    "            img[img < 0] = 0\n",
    "            img = img / np.amax(img)\n",
    "            labs = list(\n",
    "                self.trainer.datamodule.val_dataloader().dataset.class_to_idx.keys()\n",
    "            )\n",
    "            neptune_logger.experiment[\"test/misclassified_images\"].log(\n",
    "                neptune.types.File.as_image(img.transpose((1, 2, 0))),\n",
    "                description=f\"y_pred = {labs[y_pred[j]]}, y_true = {labs[y_true[j]]}\",\n",
    "            )\n",
    "\n",
    "    # Output graphs and extra metrics\n",
    "    def plot_conf_matrix(self, conf_mat):\n",
    "        labs = (\n",
    "            self.trainer.datamodule.val_dataloader().dataset.class_to_idx.keys()\n",
    "        )\n",
    "        df_cm = pd.DataFrame(conf_mat, index=labs, columns=labs)\n",
    "        plt.figure(figsize=(13, 10))\n",
    "        fig_ = sns.heatmap(\n",
    "            df_cm, annot=True, cmap=\"magma\", fmt=\"g\"\n",
    "        ).get_figure()\n",
    "        plt.close(fig_)\n",
    "        return fig_\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        conf_mat = (\n",
    "            self.val_confusion.compute().detach().cpu().numpy().astype(np.int)\n",
    "        )\n",
    "        fig_ = self.plot_conf_matrix(conf_mat)\n",
    "        self.logger.experiment[\"train/confusion_matrix\"].log(\n",
    "            File.as_image(fig_)\n",
    "        )\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        conf_mat = (\n",
    "            self.test_confusion.compute().detach().cpu().numpy().astype(np.int)\n",
    "        )\n",
    "        fig_ = self.plot_conf_matrix(conf_mat)\n",
    "        self.logger.experiment[\"test/confusion_matrix\"].log(File.as_image(fig_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Project settings/directories\n",
    "DATASET =  \"GRETI_2021-22\"\n",
    "\n",
    "PROJECT_ROOT = Path(\n",
    "    git.Repo(\".\", search_parent_directories=True).working_tree_dir\n",
    ")\n",
    "\n",
    "data_path = PROJECT_ROOT / \"data\" / \"datasets\" / DATASET / \"ML\"\n",
    "train_path, test_path = data_path / \"train\", data_path / \"test\"\n",
    "n_classes = sum([1 for i in test_path.glob(\"**/\")]) - 1\n",
    "\n",
    "\n",
    "hparams = AttrDict(\n",
    "    {\n",
    "        \"batch_size\": 64,\n",
    "        \"num_classes\": n_classes,\n",
    "        \"lr\": 0.001,\n",
    "        \"lr_scheduler_gamma\": 0.1,\n",
    "        \"milestones\": (10, 15),\n",
    "        \"transfer\": True,\n",
    "        \"train_bn\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "dm = GreatTitDataModule(\n",
    "    train_path=train_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=hparams.batch_size,\n",
    ")\n",
    "\n",
    "dm.setup()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start logger and checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To run this you will need neptune if you want to monitor model training,\n",
    "# see [2] for more details.\n",
    "\n",
    "def _init_neptune():\n",
    "    run = neptune.init(\n",
    "        mode=\"sync\",  # async won't work in Oxford HPC\n",
    "    )\n",
    "    return run\n",
    "\n",
    "def init_neptune_logger():\n",
    "    try:\n",
    "        return _init_neptune()\n",
    "    except:\n",
    "        try:\n",
    "            return _init_neptune()\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "neptune_logger = NeptuneLogger(run=init_neptune_logger())\n",
    "\n",
    "CHECKPOINTS_DIR = \"checkpoints\"\n",
    "model_checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=CHECKPOINTS_DIR,\n",
    "    monitor=\"val/acc_epoch\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=2,\n",
    "    save_weights_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate model and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ResNetClassifier(**hparams)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    logger=neptune_logger,\n",
    "    checkpoint_callback=model_checkpoint,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val/loss_epoch\", mode=\"min\", patience=4),\n",
    "        MilestonesFinetuning(milestones=(5, 10), train_bn=hparams.train_bn),\n",
    "    ],\n",
    "    log_every_n_steps=1,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    precision=16, # If supported use 16-bit precision\n",
    "    num_sanity_val_steps=0,  #FIXME #BUG @nilomr Validation gets stuck with full dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test best model in held out (test) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_model = ResNetClassifier.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "# If from fresh session:\n",
    "\n",
    "lckpt = list(\n",
    "    (\n",
    "        Path(neptune_logger.save_dir)\n",
    "        / \"Untitled\"\n",
    "        / neptune_logger._run_short_id\n",
    "        / \"checkpoints\"\n",
    "    ).glob(\"*ckpt\")\n",
    ")[-1]\n",
    "best_model = ResNetClassifier.load_from_checkpoint(lckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model predictions for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.test(best_model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract feature vectors from the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpaths = [i for i in train_path.glob(\"*/*.jpg\")]\n",
    "test_imgs = [i for i in test_path.glob(\"*/*.jpg\")]\n",
    "best_model.eval()\n",
    "best_model.to(device)\n",
    "vectors = {}\n",
    "\n",
    "# Test transformation includes random crop in the 'time' domain:\n",
    "# Running multiple times and averaging may increase robustness:\n",
    "niters = 5\n",
    "\n",
    "for i in range(niters):\n",
    "    vectors[str(i)] = {}\n",
    "    for path in tqdm(imgpaths, total=len(imgpaths)):\n",
    "        img = Image.open(path)\n",
    "        rgb_img = TimeCrop()(img)\n",
    "        tens_img = T.ToTensor()(rgb_img).unsqueeze_(0)\n",
    "        vectors[str(i)][f\"{path.stem}\"] = (\n",
    "            best_model.feature_extractor(tens_img.to(device))\n",
    "            .cpu()\n",
    "            .detach()\n",
    "            .numpy()[0, :, 0, 0]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldfs = pd.concat(\n",
    "    [pd.DataFrame.from_dict(d, orient='index') for d in vectors.values()])\n",
    "vocmean = alldfs.groupby(alldfs.index).mean()\n",
    "\n",
    "vector_dir = (PROJECT_ROOT / \"data\" / \"datasets\" / DATASET / 'ML' / \n",
    "       'output' / 'feat_vectors.csv')\n",
    "vector_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "vocmean.to_csv(vector_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "best_model.eval()\n",
    "# Predict labels for image\n",
    "predimg = list(test_path.glob(\"*/*\"))[0]\n",
    "\n",
    "img = Image.open(predimg)\n",
    "x = T.ToTensor()(img).unsqueeze_(0)\n",
    "y_hat = best_model(x)\n",
    "\n",
    "# One way\n",
    "prob = F.softmax(y_hat, dim=1)\n",
    "pred = torch.max(prob, 1)\n",
    "\n",
    "classes = list(dm.train.class_to_idx.keys())\n",
    "pred_class = classes[int(pred[1][0])]\n",
    "print(f\"true: {predimg.parent.name}, predicted: {pred_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot(imgs, with_orig=True, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0]) + with_orig\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=num_rows, ncols=num_cols, squeeze=False, figsize=(15, 8), dpi=100\n",
    "    )\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        row = [orig_img] + row if with_orig else row\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if with_orig:\n",
    "        axs[0, 0].set(title=\"Original image\")\n",
    "        axs[0, 0].title.set_size(8)\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "orig_img = Image.open(train_path / \"B227_4/B227_20210418_040000_71.jpg\")\n",
    "\n",
    "trainer.datamodule.val_dataloader()\n",
    "\n",
    "\n",
    "imgs = [Image.open(img) for img in list(train_path.rglob(\"*.jpg\"))[:4]]\n",
    "test_img = [ImgTransform().stage[\"train\"](img).permute(1, 2, 0) for img in imgs]\n",
    "\n",
    "plot(test_img[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dm.val_dataloader()):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images // 2, 2, images_so_far)\n",
    "                ax.axis(\"off\")\n",
    "                ax.set_title(f\"predicted: {dm.train.targets[preds[j]]}\")\n",
    "                plt.imshow(inputs.cpu().data[j].permute(1, 2, 0))\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)\n",
    "\n",
    "\n",
    "visualize_model(best_model.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/intelligentmachines/implementation-of-class-activation-map-cam-with-pytorch-c32f7e414923\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install grad-cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import (\n",
    "    GradCAM,\n",
    "    ScoreCAM,\n",
    "    GradCAMPlusPlus,\n",
    "    AblationCAM,\n",
    "    XGradCAM,\n",
    "    EigenCAM,\n",
    "    FullGrad,\n",
    ")\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision.models import resnet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgpaths = [i for i in train_path.glob(\"*/*.jpg\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "img_paths = random.sample(imgpaths, 50)\n",
    "out_folder = PROJECT_ROOT / \"reports\" / \"figures\" / \"activation_maps\"\n",
    "out_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for path in img_paths:\n",
    "    img = Image.open(path)\n",
    "    rgb_img = TimeCrop()(img)\n",
    "    tens_img = T.ToTensor()(rgb_img).unsqueeze_(0)\n",
    "\n",
    "    # img = Image.open(train_path / \"O30_3/O30_20210417_040000_32.jpg\")\n",
    "    # input_tensor = T.ToTensor()(img).unsqueeze_(0)\n",
    "\n",
    "    model = resnet50(pretrained=True)\n",
    "    target_layers = [model.layer4[-1]]\n",
    "\n",
    "    # target_layers = [list(best_model.backbone.children())[-1]]\n",
    "    # target_layers = [list(best_model.backbone.children())[7][-1]]\n",
    "    with GradCAM(\n",
    "        model=model, target_layers=target_layers, use_cuda=True\n",
    "    ) as cam:\n",
    "        grayscale_cam = cam(\n",
    "            input_tensor=tens_img, targets=None, eigen_smooth=True\n",
    "        )\n",
    "\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    visualization = show_cam_on_image(\n",
    "        np.float32(rgb_img) / 255, grayscale_cam, use_rgb=True\n",
    "    )\n",
    "    img = Image.fromarray(visualization)\n",
    "    img.save(out_folder / f\"{path.stem}.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = len(range(7, 11))\n",
    "\n",
    "similarNames = pd.DataFrame(index=similarityMatrix.index, columns=range(k))\n",
    "similarValues = pd.DataFrame(index=similarityMatrix.index, columns=range(k))\n",
    "for j in tqdm(range(similarityMatrix.shape[0])):\n",
    "    kSimilar = similarityMatrix.iloc[j, :].sort_values(ascending=False)[7:11]\n",
    "    similarNames.iloc[j, :] = list(kSimilar.index)\n",
    "    similarValues.iloc[j, :] = kSimilar.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def getSimilarImages(image, simNames, simVals):\n",
    "    if image in set(simNames.index):\n",
    "        imgs = list(simNames.loc[image, :])\n",
    "        vals = list(simVals.loc[image, :])\n",
    "        if image in imgs:\n",
    "            assert_almost_equal(max(vals), 1, decimal=5)\n",
    "            imgs.remove(image)\n",
    "            vals.remove(max(vals))\n",
    "        return imgs, vals\n",
    "    else:\n",
    "        print(\"'{}' Unknown image\".format(image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_similar(image):\n",
    "    imgs, values = getSimilarImages(image, similarNames, similarValues)\n",
    "    impaths = {image: [img for img in imgpaths if image in str(img)][0]}\n",
    "    for img in imgs:\n",
    "        impaths[img] = [p for p in imgpaths if img in str(p)][0]\n",
    "\n",
    "    fig = plt.figure(figsize=(30, 30))\n",
    "    grid = ImageGrid(\n",
    "        fig,\n",
    "        111,  # similar to subplot(111)\n",
    "        nrows_ncols=(1, 4),  # creates 2x2 grid of axes\n",
    "        axes_pad=0.1,  # pad between axes in inch.\n",
    "    )\n",
    "\n",
    "    for i, (ax, (im, path)) in enumerate(zip(grid, impaths.items())):\n",
    "        # Iterating over the grid returns the Axes.\n",
    "        name = im.split(\"_\")[0]\n",
    "        if i == 0:\n",
    "            ax.set_title(f\"Target: {name}\", fontsize=22)\n",
    "        else:\n",
    "            ax.set_title(f\"{name} - Value: {values[i]:.3f}\", fontsize=22)\n",
    "        ax.imshow(Image.open(path))\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for image in similarNames.index[3300:3350]:\n",
    "    plot_similar(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Measure acoustic spacbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MDS\n",
    "\n",
    "mds = MDS(\n",
    "    metric=True,\n",
    "    dissimilarity=\"precomputed\",\n",
    "    random_state=0,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "# Get the embeddings\n",
    "pts = mds.fit_transform(1 - (similarityMatrix.to_numpy() + 0.0000001))\n",
    "\n",
    "\n",
    "# Get labels\n",
    "fnames = similarityMatrix.index.values\n",
    "labels = []\n",
    "for fname in fnames:\n",
    "    for p in imgpaths:\n",
    "        if fname == p.stem:\n",
    "            labels.append(p.parent.name)\n",
    "\n",
    "pd.DataFrame(pts, labels)\n",
    "\n",
    "idx = 0\n",
    "\n",
    "for idx in range(0, 5):\n",
    "    labs = [lab if lab == list(set(labels))[idx] else \"kk\" for lab in labels]\n",
    "    # Plot the embedding, colored according to the class of the points\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    print(list(set(labels))[idx])\n",
    "    ax = sns.scatterplot(x=pts[:, 0], y=pts[:, 1], hue=labs, legend=False)\n",
    "\n",
    "# TSNE\n",
    "mx = 1 - similarityMatrix.to_numpy()\n",
    "mx = np.round((mx - np.min(mx)) / np.ptp(mx), 5)\n",
    "tsne = TSNE(n_components=2, metric=\"precomputed\")\n",
    "tsne_embedding = tsne.fit_transform(mx)\n",
    "\n",
    "for idx in range(0, 5):\n",
    "    labs = [lab if lab == list(set(labels))[idx] else \"kk\" for lab in labels]\n",
    "    # Plot the embedding, colored according to the class of the points\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    print(list(set(labels))[idx])\n",
    "    ax = sns.scatterplot(\n",
    "        x=tsne_embedding[:, 0], y=tsne_embedding[:, 1], hue=labs, legend=False\n",
    "    )\n",
    "\n",
    "# Use nearest neighbours statistics\n",
    "\n",
    "#!pip install pynndescent\n",
    "from pynndescent import NNDescent\n",
    "\n",
    "index = NNDescent(mx)\n",
    "knns = index.query(mx[4:5], k=15)\n",
    "labels[4]\n",
    "\n",
    "for i, dist in zip(knns[0][0], knns[1][0]):\n",
    "    print(i)\n",
    "    print(dist)\n",
    "    print(f\"label: {labels[i]}, dist: {dist}\")\n",
    "\n",
    "\n",
    "# Get median distance to K k nearest neighbours as measure of distinctiveness?\n",
    "# then convex hull as measure of repertoire diversity, etc\n",
    "\n",
    "\n",
    "pd.DataFrame(similarityMatrix.to_numpy(), columns=labels, index=labels)\n",
    "\n",
    "pd.DataFrame(tsne_embedding, columns=[\"x\", \"y\"], index=labels).query(\"y >60\")\n",
    "\n",
    "\n",
    "# Add the second plot\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "# Plot the points again\n",
    "plt.scatter(pts[:, 0], pts[:, 1])\n",
    "\n",
    "# Annotate each point by its corresponding face image\n",
    "for x, ind in zip(X, range(pts.shape[0])):\n",
    "    im = x.reshape(64, 64)\n",
    "    imagebox = OffsetImage(im, zoom=0.3, cmap=plt.cm.gray)\n",
    "    i = pts[ind, 0]\n",
    "    j = pts[ind, 1]\n",
    "    ab = AnnotationBbox(imagebox, (i, j), frameon=False)\n",
    "    ax.add_artist(ab)\n",
    "plt.title(title)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/domain_templates/computer_vision_fine_tuning.py\n",
    "\n",
    "[2]\n",
    "https://docs.neptune.ai/integrations/lightning/ <br>\n",
    "https://docs.neptune.ai/usage/best_practices/#configuring-your-credentials\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0956b34faf254119b200338e019e5863": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_324870f516de4da283471a6fe5cbf32d",
        "IPY_MODEL_1bc4477104cc495e91592b38610fd62c"
       ],
       "layout": "IPY_MODEL_9b7658f9bf944e5486a402ef9409843f"
      }
     },
     "18b2399586e34c0094b395fcf6dbdf67": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1bc4477104cc495e91592b38610fd62c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b52f0afe2cef4ddc85961d2136c0e77d",
       "placeholder": "​",
       "style": "IPY_MODEL_21f0010d16364031bad7198af2d6c07d",
       "value": " 12500/12500 [54:19&lt;00:00,  3.84it/s]"
      }
     },
     "21f0010d16364031bad7198af2d6c07d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "324870f516de4da283471a6fe5cbf32d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": " 90%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_18b2399586e34c0094b395fcf6dbdf67",
       "max": 12500,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_90ad8398dccc4147a8c394802d87b394",
       "value": 11281
      }
     },
     "90ad8398dccc4147a8c394802d87b394": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "9b7658f9bf944e5486a402ef9409843f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b52f0afe2cef4ddc85961d2136c0e77d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
